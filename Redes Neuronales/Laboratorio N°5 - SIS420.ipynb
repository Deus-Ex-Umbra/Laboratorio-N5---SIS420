{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOaV2wdC1ERJdB/TstD1F7E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Laboratorio N°5 - SIS420**\n","\n","**Introducción.**\n","\n","En este cuadernillo, para este laboratorio, nos enfocaremos en el entrenamiento y evaluación de redes neuronales, la primera parte tendrá la implementación manual de una red neuronal y la segunda parte tendrá la implementación de una red neuronal con PyTorch.\n","\n","**Objetivos.**\n","\n","- Preparar el dataset, usando pandas, para el entrenamiento correcto de la red neuronal.\n","- Implementar el código respectivo para la creación de una red neuronal (manual y PyTorch).\n","- Entrenar la red neuronal para la realización de predicciones (manual y PyTorch).\n","- Evaluar la efectividad de las predicciones de la red neuronal (manual y PyTorch).\n","- Establecer conclusiones y comparaciones respecto a las implementaciones."],"metadata":{"id":"nUO-bLgE-Vof"}},{"cell_type":"markdown","source":["**(0) Declaración de Librerías.** Antes que nada, declararemos las respectivas librerías que se usarán en este cuadernillo, así evitaremos redundancias y se tendrá un código más ordenado y limpio. Además se montará el drive para trabajar con los archivos de Google Drive."],"metadata":{"id":"pUwVt-VhmauR"}},{"cell_type":"markdown","source":["**Importación de bibliotecas:**\n","- import os: Importa la biblioteca os, que proporciona funciones para interactuar con el sistema operativo, como manipulación de archivos y directorios.\n","- import numpy as np: Importa la biblioteca numpy y la renombra como np. numpy es ampliamente utilizado para cálculos numéricos y manipulación de matrices.\n","- import pandas as pd: Importa la biblioteca pandas y la renombra como pd. pandas se utiliza para el análisis y manipulación de datos tabulares.\n","- from matplotlib import pyplot as plt: Importa la función pyplot de la biblioteca matplotlib y la renombra como plt.\n","- matplotlib se utiliza para crear gráficos y visualizaciones.\n","- from scipy import optimize: Importa la biblioteca scipy y su módulo optimize. scipy proporciona herramientas para optimización y análisis numérico.\n","- from sklearn.model_selection import train_test_split: Importa la función train_test_split de la biblioteca sklearn. Se utiliza para dividir conjuntos de datos en conjuntos de entrenamiento y prueba.\n","- import torch: Importa la biblioteca torch, que se utiliza para trabajar con redes neuronales y tensores.\n","- from torch import optim: Importa el módulo optim de la biblioteca torch. Proporciona algoritmos de optimización para entrenar modelos de aprendizaje profundo.\n","- from torch import nn: Importa el módulo nn de la biblioteca torch. Contiene clases para construir redes neuronales.\n","- from torch.utils.data import TensorDataset, DataLoader: Importa las clases TensorDataset y DataLoader de la biblioteca torch.utils.data. Se utilizan para cargar datos en tensores y crear lotes para el entrenamiento.\n","- import torchvision: Importa la biblioteca torchvision, que proporciona conjuntos de datos y transformaciones para la visión por computadora.\n","- import torchvision.transforms as transforms: Importa el módulo transforms de torchvision. Se utiliza para aplicar transformaciones a imágenes.\n","- import torch.nn.functional as F: Importa el módulo F de la biblioteca torch.nn. Contiene funciones de activación y otras operaciones.\n","- import torchvision.datasets as datasets: Importa el módulo datasets de torchvision. Proporciona conjuntos de datos estándar para la visión por computadora.\n","- from tqdm import tqdm: Importa la función tqdm de la biblioteca tqdm. Se utiliza para mostrar barras de progreso en bucles.\n","- from google.colab import drive: Importa la función drive de la biblioteca google.colab. Se utiliza para montar Google Drive en Google Colab.\n","- %matplotlib inline: Configura la visualización de gráficos en línea en el entorno de Google Colab.\n","**Montaje de Google Drive:**\n","La línea drive.mount('/content/gdrive') monta Google Drive en la ubicación /content/gdrive. Esto permite acceder a archivos almacenados en Google Drive desde el entorno de Colab."],"metadata":{"id":"nrPFFljKJI3v"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-lgo271n0l2q","executionInfo":{"status":"ok","timestamp":1714867101939,"user_tz":240,"elapsed":12778,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}},"outputId":"0fe67280-55b4-4c87-c2c7-b0c40eb2cb1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","from scipy import optimize\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch import optim\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","from tqdm import tqdm\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","%matplotlib inline"]},{"cell_type":"markdown","source":["# **Primera Parte**\n"],"metadata":{"id":"1qTZ64d-oyu3"}},{"cell_type":"markdown","source":["**(1) Declaración de las funciones.**"],"metadata":{"id":"wX7GJPX0o8SU"}},{"cell_type":"markdown","source":["La función *cargarDataset* se encarga de cargar y preparar un conjunto de datos para el entrenamiento y la evaluación de un modelo de Inteligencia Artificial."],"metadata":{"id":"Fqel25JpKRkh"}},{"cell_type":"code","source":["def cargarDataset(_dataset):\n","  dataset = preprocesarDataset(_dataset)\n","  train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=99)\n","  X_train, y_train = train_set.iloc[:, :-1], train_set.iloc[:, -1]\n","  X_test, y_test = test_set.iloc[:, :-1], test_set.iloc[:, -1]\n","  X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n","  X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)\n","  return X_train, X_test, y_train, y_test"],"metadata":{"id":"COC_GWJwpskl","executionInfo":{"status":"ok","timestamp":1714867101939,"user_tz":240,"elapsed":8,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["La función *preprocesarDataset* se utiliza para preparar un conjunto de datos antes de utilizarlo en un modelo de inteligencia artificial.\n","- Carga el conjunto de datos desde un archivo CSV.\n","- Identifica y procesa las columnas con datos categóricos, convirtiéndolos en valores numéricos para que puedan ser utilizados.\n","- Maneja los valores faltantes reemplazándolos con ceros.\n","- Mantiene la columna de etiquetas (‘labels’) al final del conjunto de datos para su uso posterior."],"metadata":{"id":"clMWn8N2KZX8"}},{"cell_type":"code","source":["def preprocesarDataset(_dataset):\n","  dataset = pd.read_csv(_dataset, sep=',', header=0, decimal='.')\n","  datos = {}\n","  columnas = dataset.columns[dataset.dtypes == 'object'].tolist()\n","  for columna in columnas:\n","    datos[columna] = dataset[columna].drop_duplicates().values\n","  datos_num = {}\n","  for columna, valores in datos.items():\n","    indice_reemp = 0\n","    datos_num_col = {}\n","    for valor in valores:\n","      if valor not in datos_num_col and not pd.isnull(valor):\n","        datos_num_col[valor] = indice_reemp\n","        indice_reemp += 1\n","    if np.nan not in datos_num_col:\n","      datos_num_col[np.nan] = 0\n","    datos_num[columna] = datos_num_col\n","  for columna, d_n in datos_num.items():\n","    dataset[columna] = dataset[columna].replace(d_n)\n","  dataset = dataset.fillna(0)\n","  etiquetas = dataset.pop('labels')\n","  dataset.insert(len(dataset.columns), 'labels', etiquetas)\n","  return dataset"],"metadata":{"id":"ZntoSJxDp0PQ","executionInfo":{"status":"ok","timestamp":1714867101940,"user_tz":240,"elapsed":8,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["La función *normalizarCaracteristicas* se utiliza para normalizar las características de un conjunto de datos.\n","La normalización es un paso importante en el preprocesamiento de datos para muchos algoritmos de machine learning, ya que pone todas las características en una escala común sin distorsionar las diferencias en los rangos de valores. Además, la normalización puede mejorar la convergencia de algoritmos de optimización."],"metadata":{"id":"-T7KWRNKMQ8a"}},{"cell_type":"code","source":["def normalizarCaracteristicas(_X):\n","  return _X / 255"],"metadata":{"id":"pI6Gj2KjsJpx","executionInfo":{"status":"ok","timestamp":1714867101940,"user_tz":240,"elapsed":7,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["La función *definirCapaOculta* está diseñada para determinar la estructura de una capa oculta en una red neuronal.\n","- ce representa el número de características de entrada (input features) que la red neuronal recibirá.\n","- ne es el número de clases únicas o diferentes que hay en el conjunto de etiquetas y, lo cual es importante para la clasificación.\n","- co es el número calculado de neuronas en la capa oculta.\n","\n","La fórmula utilizada es una heurística común que toma la raíz cuadrada del producto del número de características de entrada y el número de clases únicas. Esto es solo una estimación y el número óptimo de neuronas puede variar dependiendo del problema específico.\n","La razón detrás de este valor específico de epsilon_init está relacionada con la técnica de inicialización de Xavier, también conocida como \"glorot initialization\". Esta técnica se desarrolló para abordar el problema de la inicialización de los pesos en redes neuronales, donde los valores iniciales de los pesos pueden afectar significativamente la convergencia y el rendimiento del modelo durante el entrenamiento.\n","La función devuelve estos tres valores, que pueden ser utilizados para definir la arquitectura de una red neuronal."],"metadata":{"id":"UNU1pGVkMdm4"}},{"cell_type":"code","source":["def definirCapaOculta(X, y):\n","  ce = X.shape[1]\n","  ne = len(np.unique(y))\n","  co = int(np.sqrt(ce * ne))\n","  return ce, ne, co"],"metadata":{"id":"vhZxqGQrzq6f","executionInfo":{"status":"ok","timestamp":1714867101940,"user_tz":240,"elapsed":7,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["La función *inicializarPesos* se utiliza para inicializar los pesos de una red neuronal antes del entrenamiento.\n","La inicialización de los pesos es un paso crucial en la configuración de una red neuronal. Una buena inicialización puede ayudar a mejorar la velocidad de convergencia durante el entrenamiento y evitar problemas como el desvanecimiento o la explosión de gradientes. La técnica utilizada aquí es una forma de inicialización basada en la regla de Xavier, que considera el tamaño de las capas anterior y siguiente para determinar el rango de los valores aleatorios."],"metadata":{"id":"A_Ly43ABMeC3"}},{"cell_type":"code","source":["def inicializarPesos(ce, ne, co):\n","  init_bound = np.sqrt(2.0 / (ce + co))\n","  Theta1 = np.random.uniform(-init_bound, init_bound, (co, ce + 1))\n","  Theta2 = np.random.uniform(-init_bound, init_bound, (ne, co + 1))\n","  return Theta1, Theta2\n"],"metadata":{"id":"FdZ4sKu_0RZI","executionInfo":{"status":"ok","timestamp":1714867101940,"user_tz":240,"elapsed":7,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["La función *sigmoide* es una implementación de la función de activación sigmoide.\n","\n","La función sigmoide es una función matemática que tiene una característica forma de “S”. Se utiliza comúnmente como función de activación en redes neuronales porque tiene una salida que varía de 0 a 1, lo que la hace útil para problemas de clasificación binaria. Además, su derivada es fácil de calcular, lo que es útil durante el proceso de retropropagación en el entrenamiento de redes neuronales."],"metadata":{"id":"ajXwOQe1Meep"}},{"cell_type":"code","source":["def sigmoide(z):\n","  z = np.clip(z, -500, 500)\n","  return 1.0 / (1.0 + np.exp(-z))"],"metadata":{"id":"yG5oyKQA1dLx","executionInfo":{"status":"ok","timestamp":1714867101940,"user_tz":240,"elapsed":6,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["La función *sigmoideGradiente* calcula el gradiente de la función de activación sigmoide, que es útil durante el proceso de retropropagación en el entrenamiento de redes neuronales.\n","\n","Este gradiente indica cómo cambia la salida de la función sigmoide con respecto a su entrada ( z ), y es crucial para ajustar los pesos de la red neuronal durante el aprendizaje."],"metadata":{"id":"G3BZyzUzMfAN"}},{"cell_type":"code","source":["def sigmoideGradiente(z):\n","  sig = sigmoide(z)\n","  return sig * (1 - sig)"],"metadata":{"id":"pWzpiazS1iQK","executionInfo":{"status":"ok","timestamp":1714867101940,"user_tz":240,"elapsed":6,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["La función *redneuronalFuncionCosto* es una implementación de la función de costo y el gradiente para una red neuronal en el contexto de aprendizaje supervisado.\n","\n","- Se realiza un reshape de los parámetros de la red neuronal para obtener las matrices de pesos Theta1 y Theta2.\n","- Se calcula la activación de cada capa de la red utilizando la función sigmoide.\n","- Se transforma el vector de etiquetas y en una matriz binaria para la clasificación multiclase.\n","- Se añade un término de regularización para evitar el sobreajuste.\n","- Se calcula la función de costo con regularización.\n","- Se realiza la retropropagación para calcular los errores y gradientes de cada capa.\n","- Se ajustan los gradientes con un término de regularización y se devuelven junto con la función de costo."],"metadata":{"id":"bCIYEaXeMfao"}},{"cell_type":"code","source":["def redneuronalFuncionCosto(rn_p, ce, co, ne, X, y, lambda_):\n","  Theta1 = np.reshape(rn_p[:co * (ce + 1)], (co, (ce + 1)))\n","  Theta2 = np.reshape(rn_p[(co * (ce + 1)):], (ne, (co + 1)))\n","  m = y.size\n","  J = 0\n","  Theta1_grad = np.zeros(Theta1.shape)\n","  Theta2_grad = np.zeros(Theta2.shape)\n","  a1 = np.concatenate([np.ones((m, 1)), X], axis=1)\n","  a2 = sigmoide(a1.dot(Theta1.T))\n","  a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2], axis=1)\n","  a3 = sigmoide(a2.dot(Theta2.T))\n","  y_matrix = y.values.reshape(-1)\n","  y_matrix = np.eye(ne)[y_matrix]\n","  temp1 = Theta1\n","  temp2 = Theta2\n","  reg_term = (lambda_ / (2 * m)) * (np.sum(np.square(temp1[:, 1:])) + np.sum(np.square(temp2[:, 1:])))\n","  J = (-1 / m) * np.sum((np.log(a3) * y_matrix) + np.log(1 - a3) * (1 - y_matrix)) + reg_term\n","  delta_3 = a3 - y_matrix\n","  delta_2 = delta_3.dot(Theta2)[:, 1:] * sigmoideGradiente(a1.dot(Theta1.T))\n","  Delta1 = delta_2.T.dot(a1)\n","  Delta2 = delta_3.T.dot(a2)\n","  Theta1_grad = (1 / m) * Delta1\n","  Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (lambda_ / m) * Theta1[:, 1:]\n","  Theta2_grad = (1 / m) * Delta2\n","  Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (lambda_ / m) * Theta2[:, 1:]\n","  grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n","  return J, grad"],"metadata":{"id":"kY7sH87N1vSY","executionInfo":{"status":"ok","timestamp":1714867101940,"user_tz":240,"elapsed":5,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["La función *realizarPredicciones* se utiliza para hacer predicciones con una red neuronal ya entrenada.\n","\n","- Se añade un término de sesgo a las entradas y a la primera capa oculta antes de calcular la activación.\n","- Se utiliza la función sigmoide para calcular la activación de las capas ocultas.\n","- Se utiliza np.argmax para seleccionar la clase con la mayor probabilidad como la predicción final para cada ejemplo."],"metadata":{"id":"0nLApdCTMf69"}},{"cell_type":"code","source":["def realizarPredicciones(X, Theta1, Theta2):\n","  m = X.shape[0]\n","  p = np.zeros(m)\n","  h1 = sigmoide(np.dot(np.concatenate([np.ones((m, 1)), X], axis=1), Theta1.T))\n","  h2 = sigmoide(np.dot(np.concatenate([np.ones((m, 1)), h1], axis=1), Theta2.T))\n","  p = np.argmax(h2, axis=1)\n","  return p"],"metadata":{"id":"f8ziJn4D6V-I","executionInfo":{"status":"ok","timestamp":1714867101941,"user_tz":240,"elapsed":6,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["La función *calcularPrecision* se utiliza para calcular la precisión de un conjunto de predicciones comparándolas con los valores reales.\n","- Se utiliza np.round para convertir las probabilidades de las predicciones en valores binarios (0 o 1).\n","- Se calcula la precisión como el promedio de las predicciones correctas (donde las predicciones redondeadas son iguales a las etiquetas reales y_test).\n","- Se imprime la precisión en formato de porcentaje con diez decimales de precisión."],"metadata":{"id":"gP87eRhnMgS5"}},{"cell_type":"code","source":["def calcularPrecision(predicciones, y):\n","  predicciones_redondeadas = np.round(predicciones)\n","  precision = np.mean(predicciones_redondeadas == y_test) * 100\n","  print(\"Precisión de las predicciones en el conjunto de prueba: {:.10f}%\".format(precision))"],"metadata":{"id":"eOBJ8oLb614W","executionInfo":{"status":"ok","timestamp":1714867102463,"user_tz":240,"elapsed":3,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["**(2) Implementación y entrenamiento de la Red Neuronal.**"],"metadata":{"id":"dY8gEgQfpCl5"}},{"cell_type":"markdown","source":["Este fragmento de código está utilizando las funciones previamente definidas para cargar un conjunto de datos desde un archivo CSV y luego normalizar las características de los conjuntos de entrenamiento y prueba."],"metadata":{"id":"gJz0_WdXMgy6"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = cargarDataset('/content/gdrive/MyDrive/SIS420/Laboratorio-N5_-_SIS420/Datasets/TMNIST Alphabet/94_character_TMNIST.csv')\n","x_train = normalizarCaracteristicas(X_train)\n","X_test = normalizarCaracteristicas(X_test)"],"metadata":{"id":"AC1K0IRiwp2w","executionInfo":{"status":"ok","timestamp":1714864183667,"user_tz":240,"elapsed":109628,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Este fragmento de código está configurando una red neuronal, definiendo la capa oculta y los pesos iniciales.\n","1. Se llama a la función definirCapaOculta para determinar el número de características de entrada, el número de etiquetas (clases) y el tamaño de la capa oculta basándose en los datos de entrenamiento.\n","2. Luego, se utiliza la función inicializarPesos para crear dos matrices de pesos (Theta1 y Theta2) con valores iniciales aleatorios adecuados para la red neuronal.\n","3. Finalmente, se aplanan estas matrices en vectores unidimensionales y se concatenan para formar un solo vector llamado redneuronal_parametros. Este vector contiene todos los parámetros (pesos) de la red neuronal que serán ajustados durante el entrenamiento."],"metadata":{"id":"QO6f_ozxMhSF"}},{"cell_type":"code","source":["caracteristicas_entrada, numero_etiquetas, capa_oculta = definirCapaOculta(X_train, y_train)\n","Theta1, Theta2 = inicializarPesos(caracteristicas_entrada, numero_etiquetas, capa_oculta)\n","redneuronal_parametros = np.concatenate([Theta1.ravel(), Theta2.ravel()])"],"metadata":{"id":"sL0vgbexDerO","executionInfo":{"status":"ok","timestamp":1714864183667,"user_tz":240,"elapsed":33,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Este fragmento de código está utilizando una función de costo para entrenar una red neuronal y luego obtener los parámetros optimizados.\n","1. Se crea una función lambda que encapsula la función de costo de la red neuronal. Esta función se pasará al algoritmo de optimización.\n","2. Se llama a optimize.minimize con la función de costo, los parámetros iniciales de la red neuronal, y se especifican opciones adicionales para el proceso de optimización.\n","3. Después de la optimización, se reestructuran los parámetros optimizados en las matrices de pesos Theta1 y Theta2 que corresponden a las capas de la red neuronal."],"metadata":{"id":"tSI3TK1oMh6S"}},{"cell_type":"code","source":["funcionCosto = lambda p: redneuronalFuncionCosto(p, caracteristicas_entrada, capa_oculta, numero_etiquetas, X_train, y_train, 0.1)\n","redneuronal_parametros = optimize.minimize(funcionCosto, redneuronal_parametros, jac=True, method='L-BFGS-B', options={'disp': True, 'maxiter': 100}).x\n","Theta1 = np.reshape(redneuronal_parametros[:capa_oculta * (caracteristicas_entrada + 1)], (capa_oculta, (caracteristicas_entrada + 1)))\n","Theta2 = np.reshape(redneuronal_parametros[(capa_oculta * (caracteristicas_entrada + 1)):], (numero_etiquetas, (capa_oculta + 1)))"],"metadata":{"id":"i3aXcyddDh8T","executionInfo":{"status":"ok","timestamp":1714865983149,"user_tz":240,"elapsed":653600,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["**(3)Cálculo de la presición.**"],"metadata":{"id":"ntSjrllBpHOf"}},{"cell_type":"markdown","source":["Se realizan las predicciones y el posterior cálculo de la precisión del modelo para verificar su efectividad."],"metadata":{"id":"cp9_VsjWMjd0"}},{"cell_type":"code","source":["calcularPrecision(realizarPredicciones(X_test, Theta1, Theta2), y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kkc6t5yS73TX","executionInfo":{"status":"ok","timestamp":1714866888559,"user_tz":240,"elapsed":1815,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}},"outputId":"a8d1a283-fc40-482f-ef08-bc10646df9dd"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Precisión de las predicciones en el conjunto de prueba: 80.3480545066%\n"]}]},{"cell_type":"markdown","source":["*Nota: Se establecieron un nivel bajo de iteraciones, esto debido a que el optimize.minimize tarda al menos 4 minutos promedio en solamente 10 iteraciones, esto debido a que el tamaño del dataset es demasiado grande.*"],"metadata":{"id":"2j3zyJMwuxuO"}},{"cell_type":"markdown","source":["# **Segunda Parte**"],"metadata":{"id":"UwkPULk7pQP0"}},{"cell_type":"markdown","source":["**(1) Declaración de las funciones**"],"metadata":{"id":"-tKEPpZ180XB"}},{"cell_type":"markdown","source":["La clase *RedNeuronalMLS* es una definición de una red neuronal simple utilizando PyTorch, una biblioteca de aprendizaje profundo.\n","* ce representa el número de características de entrada.\n","* ne es el número de etiquetas de salida (por ejemplo, clases en un problema de clasificación).\n","* co es el número de neuronas en la capa oculta, calculado como la raíz cuadrada del producto de ce y ne.\n","* nn.Linear es una capa que aplica una transformación lineal a los datos entrantes: ( y = xA^T + b ).\n","* F.sigmoid es una función de activación que transforma los valores de entrada en un rango entre 0 y 1.\n","\n","Es importante mencionar que la función de activación F.sigmoid está en desuso en versiones más recientes de PyTorch, y se recomienda usar torch.sigmoid o F.relu en su lugar para mejores resultados y prácticas actualizadas."],"metadata":{"id":"-zvinNw_MkTL"}},{"cell_type":"code","source":["class RedNeuronalMLS(nn.Module):\n","    def __init__(self, ce, ne):\n","        super(RedNeuronalMLS, self).__init__()\n","        co = int(np.sqrt(ce * ne))\n","        self.capa_oculta = nn.Linear(ce, co)\n","        self.capa_salida = nn.Linear(co, ne)\n","    def forward(self, x):\n","        x = self.capa_oculta(x)\n","        x = F.sigmoid(x)\n","        x = self.capa_salida(x)\n","        return x"],"metadata":{"id":"52vCeLSx9Bum","executionInfo":{"status":"ok","timestamp":1714867108641,"user_tz":240,"elapsed":451,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["La función *calcularPrecision* se utiliza para evaluar la precisión de un modelo de red neuronal en PyTorch.\n","* Se itera sobre un loader, que generalmente es un DataLoader de PyTorch que proporciona lotes de datos y etiquetas.\n","* Se utiliza modelo.eval() para poner el modelo en modo de evaluación, lo que es importante para algunas capas como dropout o batch normalization que tienen comportamientos diferentes durante el entrenamiento y la evaluación.\n","* Se utiliza torch.no_grad() para desactivar el cálculo de gradientes, ya que no es necesario durante la evaluación y ahorra memoria y tiempo de cómputo.\n","* Se calculan las predicciones y se compara con las etiquetas verdaderas y para determinar la cantidad de predicciones correctas.\n","* Finalmente, se devuelve la precisión calculada como un porcentaje y la lista de predicciones."],"metadata":{"id":"8Sxh3fn_MkyD"}},{"cell_type":"code","source":["def calcularPrecisionModel(loader, modelo):\n","  num_correct = 0\n","  num_samples = 0\n","  modelo.eval()\n","  predicciones = []\n","  with torch.no_grad():\n","      for x, y in loader:\n","          x = x.to(device=dispositivo)\n","          y = y.to(device=dispositivo)\n","          x = x.reshape(x.shape[0], -1)\n","          scores = modelo(x)\n","          _, predictions = scores.max(1)\n","          predicciones.append(predictions)\n","          num_correct += (predictions == y).sum()\n","          num_samples += predictions.size(0)\n","  modelo.train()\n","  return num_correct/num_samples, predicciones"],"metadata":{"id":"QidFRraTEzSB","executionInfo":{"status":"ok","timestamp":1714867110123,"user_tz":240,"elapsed":350,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["**(2) Implementación de la Red Neuronal.**"],"metadata":{"id":"1MmFtbFF_MXY"}},{"cell_type":"markdown","source":["Este fragmento de código prepara los datos para ser utilizados en un entorno de PyTorch, seleccionando el dispositivo de cómputo y convirtiendo los conjuntos de datos en tensores y datasets de PyTorch.\n","* Se utiliza torch.device para seleccionar automáticamente CUDA si está disponible, lo que permite el uso de GPU para el entrenamiento, de lo contrario, se utiliza la CPU.\n","* Se llama a la función cargarDataset para obtener los conjuntos de datos divididos en entrenamiento y prueba.\n","* Se convierten los conjuntos de datos en tensores de PyTorch, que son estructuras de datos compatibles con las operaciones de PyTorch y se pueden transferir al dispositivo seleccionado (CPU o GPU).\n","* Se crean datasets de PyTorch (TensorDataset) que emparejan las entradas con sus etiquetas correspondientes. Estos datasets se pueden utilizar para cargar los datos en un DataLoader para iterar durante el entrenamiento y la evaluación."],"metadata":{"id":"xmCrLUI5Mlen"}},{"cell_type":"code","source":["dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","X_train, X_test, y_train, y_test = cargarDataset('/content/gdrive/MyDrive/SIS420/Laboratorio-N5_-_SIS420/Datasets/TMNIST Alphabet/94_character_TMNIST.csv')\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n","y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.int64)\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"],"metadata":{"id":"iU84I03B_juz","executionInfo":{"status":"ok","timestamp":1714867216710,"user_tz":240,"elapsed":103575,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Este fragmento de código configura los parámetros para entrenar un modelo de red neuronal utilizando PyTorch.\n","* caracteristicas_entrada es el número de características (o atributos) que cada muestra de entrada tiene.\n","* numero_etiquetas es el número de clases diferentes que el modelo intentará predecir.\n","* tasa_aprendizaje es un hiperparámetro que controla cuánto se ajustan los pesos del modelo con respecto al gradiente del error en cada actualización.\n","* tamaño_lote y numero_iteraciones son hiperparámetros que controlan el tamaño de los lotes de datos procesados y el número de veces que se entrenará el modelo en todo el conjunto de datos, respectivamente.\n","* train_loader y test_loader son objetos de PyTorch que automatizan el proceso de iteración sobre los conjuntos de datos durante el entrenamiento y la evaluación, respectivamente."],"metadata":{"id":"bnd7_GWjMmFV"}},{"cell_type":"code","source":["caracteristicas_entrada = X_train.shape[1]\n","numero_etiquetas = len(np.unique(y_train))\n","tasa_aprendizaje = 0.001\n","tamaño_lote = 10000\n","numero_iteraciones = 9\n","train_loader = DataLoader(train_dataset, batch_size=tamaño_lote, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=tamaño_lote, shuffle=False)"],"metadata":{"id":"pYtC0BbvJ_yS","executionInfo":{"status":"ok","timestamp":1714867216712,"user_tz":240,"elapsed":29,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Este fragmento de código establece y entrena un modelo de red neuronal utilizando la biblioteca PyTorch.\n","* Se inicializa el modelo de la red neuronal y se configura para usar el dispositivo adecuado (CPU o GPU).\n","* Se utiliza la entropía cruzada como función de pérdida, que es adecuada para tareas de clasificación multiclase.\n","* Se elige Adam como el optimizador, que ajustará los pesos del modelo para minimizar la función de pérdida.\n","* El bucle for externo itera sobre las epoch, y el bucle for interno itera sobre los lotes de datos proporcionados por train_loader.\n","* En cada iteración, se calculan las puntuaciones del modelo, se calcula la pérdida, se limpian los gradientes antiguos, se realiza la retropropagación para obtener nuevos gradientes, y finalmente se actualizan los pesos del modelo."],"metadata":{"id":"z0srY55SMmix"}},{"cell_type":"code","source":["modelo = RedNeuronalMLS(caracteristicas_entrada, numero_etiquetas).to(dispositivo)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(modelo.parameters(), lr=tasa_aprendizaje)\n","for epoch in range(numero_iteraciones):\n","    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n","        data = data.to(device=dispositivo)\n","        targets = targets.to(device=dispositivo)\n","        data = data.reshape(data.shape[0], -1)\n","        scores = modelo(data)\n","        loss = criterion(scores, targets)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kl6zBHKKEad","executionInfo":{"status":"ok","timestamp":1714867421360,"user_tz":240,"elapsed":203231,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}},"outputId":"914cbcff-d22d-4ccb-915e-eb434addced0"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 22/22 [00:22<00:00,  1.03s/it]\n","100%|██████████| 22/22 [00:22<00:00,  1.02s/it]\n","100%|██████████| 22/22 [00:21<00:00,  1.02it/s]\n","100%|██████████| 22/22 [00:22<00:00,  1.03s/it]\n","100%|██████████| 22/22 [00:22<00:00,  1.03s/it]\n","100%|██████████| 22/22 [00:22<00:00,  1.04s/it]\n","100%|██████████| 22/22 [00:23<00:00,  1.06s/it]\n","100%|██████████| 22/22 [00:22<00:00,  1.04s/it]\n","100%|██████████| 22/22 [00:22<00:00,  1.01s/it]\n"]}]},{"cell_type":"markdown","source":["**(3) Cálculo de la presición.**"],"metadata":{"id":"u4Ld5K9W_R0-"}},{"cell_type":"markdown","source":["Este fragmento de código utiliza la función calcularPrecision para evaluar la precisión del modelo de red neuronal tanto en el conjunto de entrenamiento como en el de prueba."],"metadata":{"id":"HU4kImTrMm_J"}},{"cell_type":"code","source":["p_train, pred_train = calcularPrecisionModel(train_loader, modelo)\n","p_test, pred_test = calcularPrecisionModel(test_loader, modelo)\n","print(f\"Presición en el dataset de entrenamiento: {p_train*100:.2f}\")\n","print(f\"Presición en el dataset de prueba: {p_test*100:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zere7QNjFLWX","executionInfo":{"status":"ok","timestamp":1714867436348,"user_tz":240,"elapsed":15016,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}},"outputId":"8817a71e-38a0-486a-8ce2-5d5d594d1794"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Presición en el dataset de entrenamiento: 85.56\n","Presición en el dataset de prueba: 84.89\n"]}]}]}